{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de textos con Deep Learning\n",
    "\n",
    "Las redes neuronales recurrentes también pueden usarse como modelos generativos. Esto significa que, además de ser utilizados para modelos predictivos, pueden aprender las secuencias de un problema y luego generar secuencias plausibles completamente nuevas para el dominio del problema. Los modelos generativos son útiles no solo para estudiar qué tan bien un modelo ha aprendido un problema, sino para aprender más sobre el dominio del problema en sí mismo. En este proyecto, se verá cómo crear un modelo generativo de texto, carácter por carácter, utilizando las redes neuronales recurrentes de LSTM en Python con Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Descripción del problema: Generación de texto**\n",
    "Muchos de los textos clásicos ya no están protegidos por derechos de autor. Esto significa que se pueden descargar todo el texto de estos libros de forma gratuita y utilizarlos en experimentos, como crear modelos generativos. En este caso vamos a utilizar un libro de la infancia como el conjunto de datos: Los tres cerditos.\n",
    "Vamos a aprender las dependencias entre los caracteres y las probabilidades condicionales de los caracteres en las secuencias para que a su vez podamos generar secuencias de caracteres totalmente nuevas y originales. \n",
    "\n",
    "## **¿Qué utilidad tienen los modelos generativos?**\n",
    "Estos modelos permiten en base a un conjunto de datos apreder y generar datos siguiendo secuencias aprendidas bajo los datos. Esto se utiliza mucho en **phising**. En base a mesajes preestablecidos, se generan emails, cuentas de usuarios, mensajes en aplicaciones de mensajería... etc.. para engañar al usuarios simulando que se encuentra en una conversión real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Desarrollar una pequeña Red Neuronal LSTM Recurrente**\n",
    "Empezaremos a preparar el conjunto de datos para el modelado. Nuestro libro de ejemplo no tiene encabezado o pie de pagina, pero en caso de que lo tenga deberemos eliminarlo.\n",
    "\n",
    "Comenzaremos desarrollando una sencilla red LSTM para aprender secuencias de caracteres de Los tres cerditos. En este punto usaremos este modelo para generar nuevas secuencias de caracteres. Empecemos por importar las clases y funciones que pretendemos usar para entrenar a nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación tendremos que cargar el texto ASCII del libro en la memoria y convertir todos los caracteres a minúsculas para reducir el vocabulario que la red debe aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el texto y lo pasamos a minuscula\n",
    "filename = \"los3.txt\";\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que el libro está cargado, debemos preparar los datos para el modelado de la red neuronal. No podemos modelar los caracteres directamente, sino que debemos convertirlos en enteros. Podemos hacer esto fácilmente creando primero un conjunto de todos los caracteres distintos en el libro, y luego creando un mapa de cada carácter con un número entero único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, la lista de caracteres únicos en minúsculas ordenados en el libro es la siguiente:  \n",
    "  \n",
    "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']',\n",
    "'_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef'].  \n",
    "  \n",
    "Podemos ver que puede haber algunos caracteres que podríamos eliminar para limpiar aún más el conjunto de datos reduciendo el vocabulario y mejorando el proceso de modelado. Ahora que el libro ha sido cargado y el mapeo preparado, podemos resumir el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de caracteres: 3139\n",
      "Total vocales: 38\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total de caracteres:\", n_chars)\n",
    "print(\"Total vocales:\", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el libro tiene algo menos de 3139 caracteres y que cuando se convierte a minúsculas sólo hay 38 caracteres distintos en el vocabulario para que la red aprenda. Mucho más que los 27 del alfabeto. Ahora tenemos que analizar los datos de formación para la red. Hay mucha flexibilidad en la forma que se decide dividir el texto y exponerlo a la red durante la formación. Aquí lo dividiremos en las siguientes secciones con una longitud fija de 100 caracteres, una longitud arbitraria. Podríamos fácilmente dividir los datos por frases y rellenar las secuencias más cortas y truncar las más largas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada patrón de formación de la red se compone de 100 pasos de tiempo de un carácter (X) seguido de la salida de un carácter (y). Al crear estas secuencias, deslizamos esta ventana a lo largo de todo el libro, un carácter a la vez, lo que permite que cada carácter tenga la oportunidad de aprender de los 100 caracteres que lo precedieron (excepto los primeros 100 caracteres, por supuesto). Por ejemplo, si la longitud de la secuencia es 5 (por simplicidad) entonces los dos primeros patrones de entrenamiento serían los siguientes:\n",
    "\n",
    "CHAPT -> E  \n",
    "HAPTE -> R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que dividimos el libro en estas secuencias, convertimos los caracteres en enteros usando nuestra tabla de búsqueda que preparamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patrones:  3039\n"
     ]
    }
   ],
   "source": [
    "#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = raw_text[i:i + seq_length]\n",
    "  seq_out = raw_text[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patrones: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ejecución del código hasta este punto nos muestra que cuando dividimos el conjunto de datos en datos de formación para que la red se entere de que tenemos algo menos de Total Patterns: 3039 patrones de formación. Esto tiene sentido ya que excluyendo los primeros 100 caracteres, tenemos un patrón de entrenamiento para predecir cada uno de los caracteres restantes.\n",
    "\n",
    "  \n",
    "Ahora que hemos preparado nuestros datos de entrenamiento, necesitamos transformarlos para que sean adecuados para su uso con Keras. Primero debemos transformar la lista de secuencias de entrada en la forma[muestras, pasos de tiempo, características] esperada por una red LSTM. A continuación necesitamos reescalar los números enteros al rango 0-a-1 para hacer que los patrones sean más fáciles de aprender por la red LSTM que utiliza la función de activación sigmoide por defecto.  \n",
    "  \n",
    "Finalmente, necesitamos convertir los patrones de salida (caracteres individuales convertidos en enteros) en una codificación en caliente. Esto es para que podamos configurar la red para predecir la probabilidad de cada uno de los 47 caracteres diferentes en el vocabulario (una representación más fácil) en lugar de tratar de forzarlo a predecir con precisión el siguiente carácter. Cada valor de y se convierte en un vector disperso con una longitud de 38, lleno de ceros excepto con un 1 en la columna para la letra (entero) que el patrón representa. Por ejemplo, cuando n (valor entero 31) es un valor codificado en caliente, se ve como como sigue:\n",
    "\n",
    "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  \n",
    "0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.  \n",
    "0. 0. 0. 0. 0. 0. 0. 0.]  \n",
    "  \n",
    "Podemos implementar estos pasos como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remodelar X para que sea [muestras, pasos de tiempo, características]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "#Normalizacion\n",
    "X = X / float(n_vocab)\n",
    "#Codificacion en caliente con la variable de salida\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos definir nuestro modelo LSTM. Aquí definimos una única capa LSTM oculta con 256 unidades de memoria. La red utiliza el abandono con una probabilidad del 20%. La capa de salida es una capa densa que utiliza la función de activación de softmax para generar una predicción de probabilidad para cada uno de los 38 caracteres entre 0 y 1. El problema es en realidad un problema de clasificación de un solo carácter con 38 clases y, como tal, se define como la optimización del pérdida de registro (entropía cruzada), utilizando el algoritmo de optimización de ADAM para la velocidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alejandromagdalenanino/Anaconda/anaconda3/envs/data/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Define el LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existe un conjunto de datos de prueba. Estamos modelando todo el conjunto de datos de entrenamiento para aprender la probabilidad de cada carácter en una secuencia. No nos interesa el modelo más preciso (precisión de clasificación) del conjunto de datos de formación. Este sería un modelo que predice cada carácter en el conjunto de datos de entrenamiento perfectamente. En su lugar, estamos interesados en una generalización del conjunto de datos que minimice la función de pérdida elegida. Buscamos un equilibrio entre la generalización y la adaptación, pero sin memorizar.\n",
    "La red tarda en entrenarse (unos 35 segundos por epoch en mi pc usando la CPU). Dada la lentitud y debido a nuestros requisitos de optimización, utilizaremos el modelo de checkpointing para registrar todos los pesos de la red cada vez que se observe una mejora en las pérdidas en el fin de la epoch. Usaremos el mejor conjunto de pesos (la pérdida más baja) para instanciar nuestro sistema generativo en el siguiente punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se define el checkpoint\n",
    "filepath=\"pesos-los3-30-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos ajustar nuestro modelo a los datos. Aquí utilizamos 50 epoch y un tamaño de lote grande de 128 patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alejandromagdalenanino/Anaconda/anaconda3/envs/data/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "3039/3039 [==============================] - 8s 3ms/step - loss: 3.1921\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.19206, saving model to pesos-los3-30-01-3.1921.hdf5\n",
      "Epoch 2/50\n",
      "3039/3039 [==============================] - 8s 3ms/step - loss: 2.9909\n",
      "\n",
      "Epoch 00002: loss improved from 3.19206 to 2.99090, saving model to pesos-los3-30-02-2.9909.hdf5\n",
      "Epoch 3/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9841\n",
      "\n",
      "Epoch 00003: loss improved from 2.99090 to 2.98413, saving model to pesos-los3-30-03-2.9841.hdf5\n",
      "Epoch 4/50\n",
      "3039/3039 [==============================] - 8s 2ms/step - loss: 2.9739\n",
      "\n",
      "Epoch 00004: loss improved from 2.98413 to 2.97385, saving model to pesos-los3-30-04-2.9739.hdf5\n",
      "Epoch 5/50\n",
      "3039/3039 [==============================] - 8s 3ms/step - loss: 2.9695\n",
      "\n",
      "Epoch 00005: loss improved from 2.97385 to 2.96953, saving model to pesos-los3-30-05-2.9695.hdf5\n",
      "Epoch 6/50\n",
      "3039/3039 [==============================] - 8s 3ms/step - loss: 2.9757\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.96953\n",
      "Epoch 7/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9634\n",
      "\n",
      "Epoch 00007: loss improved from 2.96953 to 2.96344, saving model to pesos-los3-30-07-2.9634.hdf5\n",
      "Epoch 8/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9646\n",
      "\n",
      "Epoch 00008: loss did not improve from 2.96344\n",
      "Epoch 9/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9674\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.96344\n",
      "Epoch 10/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9620\n",
      "\n",
      "Epoch 00010: loss improved from 2.96344 to 2.96196, saving model to pesos-los3-30-10-2.9620.hdf5\n",
      "Epoch 11/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9614\n",
      "\n",
      "Epoch 00011: loss improved from 2.96196 to 2.96138, saving model to pesos-los3-30-11-2.9614.hdf5\n",
      "Epoch 12/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9593\n",
      "\n",
      "Epoch 00012: loss improved from 2.96138 to 2.95926, saving model to pesos-los3-30-12-2.9593.hdf5\n",
      "Epoch 13/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9585\n",
      "\n",
      "Epoch 00013: loss improved from 2.95926 to 2.95846, saving model to pesos-los3-30-13-2.9585.hdf5\n",
      "Epoch 14/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9497\n",
      "\n",
      "Epoch 00014: loss improved from 2.95846 to 2.94974, saving model to pesos-los3-30-14-2.9497.hdf5\n",
      "Epoch 15/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9601\n",
      "\n",
      "Epoch 00015: loss did not improve from 2.94974\n",
      "Epoch 16/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9530\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.94974\n",
      "Epoch 17/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9510\n",
      "\n",
      "Epoch 00017: loss did not improve from 2.94974\n",
      "Epoch 18/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9573\n",
      "\n",
      "Epoch 00018: loss did not improve from 2.94974\n",
      "Epoch 19/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9460\n",
      "\n",
      "Epoch 00019: loss improved from 2.94974 to 2.94602, saving model to pesos-los3-30-19-2.9460.hdf5\n",
      "Epoch 20/50\n",
      "3039/3039 [==============================] - 9s 3ms/step - loss: 2.9442\n",
      "\n",
      "Epoch 00020: loss improved from 2.94602 to 2.94419, saving model to pesos-los3-30-20-2.9442.hdf5\n",
      "Epoch 21/50\n",
      "3039/3039 [==============================] - 10s 3ms/step - loss: 2.9404\n",
      "\n",
      "Epoch 00021: loss improved from 2.94419 to 2.94035, saving model to pesos-los3-30-21-2.9404.hdf5\n",
      "Epoch 22/50\n",
      "3039/3039 [==============================] - 10s 3ms/step - loss: 2.9315\n",
      "\n",
      "Epoch 00022: loss improved from 2.94035 to 2.93155, saving model to pesos-los3-30-22-2.9315.hdf5\n",
      "Epoch 23/50\n",
      "3039/3039 [==============================] - 11s 4ms/step - loss: 2.9204\n",
      "\n",
      "Epoch 00023: loss improved from 2.93155 to 2.92041, saving model to pesos-los3-30-23-2.9204.hdf5\n",
      "Epoch 24/50\n",
      "3039/3039 [==============================] - 11s 4ms/step - loss: 2.9037\n",
      "\n",
      "Epoch 00024: loss improved from 2.92041 to 2.90374, saving model to pesos-los3-30-24-2.9037.hdf5\n",
      "Epoch 25/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.8859\n",
      "\n",
      "Epoch 00025: loss improved from 2.90374 to 2.88590, saving model to pesos-los3-30-25-2.8859.hdf5\n",
      "Epoch 26/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.8722\n",
      "\n",
      "Epoch 00026: loss improved from 2.88590 to 2.87220, saving model to pesos-los3-30-26-2.8722.hdf5\n",
      "Epoch 27/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.8480\n",
      "\n",
      "Epoch 00027: loss improved from 2.87220 to 2.84796, saving model to pesos-los3-30-27-2.8480.hdf5\n",
      "Epoch 28/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.8326\n",
      "\n",
      "Epoch 00028: loss improved from 2.84796 to 2.83260, saving model to pesos-los3-30-28-2.8326.hdf5\n",
      "Epoch 29/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.8015\n",
      "\n",
      "Epoch 00029: loss improved from 2.83260 to 2.80154, saving model to pesos-los3-30-29-2.8015.hdf5\n",
      "Epoch 30/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.7948\n",
      "\n",
      "Epoch 00030: loss improved from 2.80154 to 2.79477, saving model to pesos-los3-30-30-2.7948.hdf5\n",
      "Epoch 31/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.7863\n",
      "\n",
      "Epoch 00031: loss improved from 2.79477 to 2.78630, saving model to pesos-los3-30-31-2.7863.hdf5\n",
      "Epoch 32/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.7615\n",
      "\n",
      "Epoch 00032: loss improved from 2.78630 to 2.76155, saving model to pesos-los3-30-32-2.7615.hdf5\n",
      "Epoch 33/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.7431\n",
      "\n",
      "Epoch 00033: loss improved from 2.76155 to 2.74314, saving model to pesos-los3-30-33-2.7431.hdf5\n",
      "Epoch 34/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.7254\n",
      "\n",
      "Epoch 00034: loss improved from 2.74314 to 2.72543, saving model to pesos-los3-30-34-2.7254.hdf5\n",
      "Epoch 35/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.7024\n",
      "\n",
      "Epoch 00035: loss improved from 2.72543 to 2.70240, saving model to pesos-los3-30-35-2.7024.hdf5\n",
      "Epoch 36/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.6671\n",
      "\n",
      "Epoch 00036: loss improved from 2.70240 to 2.66710, saving model to pesos-los3-30-36-2.6671.hdf5\n",
      "Epoch 37/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.6459\n",
      "\n",
      "Epoch 00037: loss improved from 2.66710 to 2.64591, saving model to pesos-los3-30-37-2.6459.hdf5\n",
      "Epoch 38/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.6477\n",
      "\n",
      "Epoch 00038: loss did not improve from 2.64591\n",
      "Epoch 39/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.5984\n",
      "\n",
      "Epoch 00039: loss improved from 2.64591 to 2.59840, saving model to pesos-los3-30-39-2.5984.hdf5\n",
      "Epoch 40/50\n",
      "3039/3039 [==============================] - 12s 4ms/step - loss: 2.5576\n",
      "\n",
      "Epoch 00040: loss improved from 2.59840 to 2.55758, saving model to pesos-los3-30-40-2.5576.hdf5\n",
      "Epoch 41/50\n",
      "3039/3039 [==============================] - 13s 4ms/step - loss: 2.5238\n",
      "\n",
      "Epoch 00041: loss improved from 2.55758 to 2.52380, saving model to pesos-los3-30-41-2.5238.hdf5\n",
      "Epoch 42/50\n",
      "3039/3039 [==============================] - 13s 4ms/step - loss: 2.4719\n",
      "\n",
      "Epoch 00042: loss improved from 2.52380 to 2.47194, saving model to pesos-los3-30-42-2.4719.hdf5\n",
      "Epoch 43/50\n",
      "3039/3039 [==============================] - 13s 4ms/step - loss: 2.4300\n",
      "\n",
      "Epoch 00043: loss improved from 2.47194 to 2.43001, saving model to pesos-los3-30-43-2.4300.hdf5\n",
      "Epoch 44/50\n",
      "3039/3039 [==============================] - 13s 4ms/step - loss: 2.3853\n",
      "\n",
      "Epoch 00044: loss improved from 2.43001 to 2.38532, saving model to pesos-los3-30-44-2.3853.hdf5\n",
      "Epoch 45/50\n",
      "3039/3039 [==============================] - 13s 4ms/step - loss: 2.3430\n",
      "\n",
      "Epoch 00045: loss improved from 2.38532 to 2.34301, saving model to pesos-los3-30-45-2.3430.hdf5\n",
      "Epoch 46/50\n",
      "3039/3039 [==============================] - 14s 4ms/step - loss: 2.3218\n",
      "\n",
      "Epoch 00046: loss improved from 2.34301 to 2.32184, saving model to pesos-los3-30-46-2.3218.hdf5\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3039/3039 [==============================] - 13s 4ms/step - loss: 2.2626\n",
      "\n",
      "Epoch 00047: loss improved from 2.32184 to 2.26258, saving model to pesos-los3-30-47-2.2626.hdf5\n",
      "Epoch 48/50\n",
      "3039/3039 [==============================] - 14s 5ms/step - loss: 2.2267\n",
      "\n",
      "Epoch 00048: loss improved from 2.26258 to 2.22668, saving model to pesos-los3-30-48-2.2267.hdf5\n",
      "Epoch 49/50\n",
      "3039/3039 [==============================] - 14s 5ms/step - loss: 2.1812\n",
      "\n",
      "Epoch 00049: loss improved from 2.22668 to 2.18122, saving model to pesos-los3-30-49-2.1812.hdf5\n",
      "Epoch 50/50\n",
      "3039/3039 [==============================] - 14s 5ms/step - loss: 2.1367\n",
      "\n",
      "Epoch 00050: loss improved from 2.18122 to 2.13666, saving model to pesos-los3-30-50-2.1367.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa2d8d031d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ajuste del modelo\n",
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis ver resultados diferentes debido a la naturaleza estocástica del modelo, y porque la dificultad de la semilla aleatoria en los modelos LSTM en obtener resultados 100% reproducibles. Esto no es una preocupación para este modelo generativo. Después de ejecutar el ejemplo, deberíais tener un número de puntos de control de peso en el directorio local. Podríamos borrarlos todos excepto el que tenga el menor valor de pérdida. Por ejemplo, cuando ejecuté este ejemplo, debajo estaba el punto de control con la menor pérdida que logré.  \n",
    "  \n",
    "**pesos-los3-30-50-2.1798.hdf5**  \n",
    "  \n",
    "La pérdida de la red disminuyó casi todas las epoch y esperó que la red pueda dejar de entrenar por muchas más epoch. En el siguiente punto veremos el uso de este modelo para generar nuevas secuencias de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de texto con una Red LSTM\n",
    "La generación de texto utilizando la red LSTM es relativamente sencilla. En primer lugar, se cargan los datos y se define la red exactamente de la misma manera que hemos visto en el punto anterior, excepto que los pesos de la red se cargan desde un punto de control y la red no necesita ser entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de los pesos de la red\n",
    "filename=\"pesos-los3-30-50-2.1367.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, al preparar el mapeo de caracteres únicos a enteros, también debemos crear un mapeo inverso que podamos usar para convertir los enteros de nuevo en caracteres para que podamos entender las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, tenemos que hacer predicciones. La forma más sencilla de usar el modelo LSTM de Keras para hacer predicciones es comenzar primero con una secuencia de semillas como entrada, generar el siguiente carácter y luego actualizar la secuencia de semillas para añadir el carácter generado al final y recortar el primer carácter. Este proceso se repite mientras queramos predecir nuevos caracteres (por ejemplo, una secuencia de 1.000 caracteres de longitud). Podemos elegir un patrón de entrada aleatorio como nuestra semilla y luego imprimir los caracteres generados a medida que los generamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semilla:\n",
      "\"  hacia el lago. los cerditos no lo volvieron a ver.  el mayor de ellos regañó a los otros dos por ha \"\n",
      "soi ae lasiia d  ln dn  no  no  no le casoita de la ee casiiaad  e ledee  e leddno  e madee  a ¡uuién teme al lobo feroz, al lobo  al lobo!  - ¡quién teme al lobo feroz, al lobo feroz! a  cnuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡uuién teme al lobo feroz, al lobo feroz! -  ¡u\n",
      "Hecho.\n"
     ]
    }
   ],
   "source": [
    "#Toma una semilla aleatoria\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Semilla:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "#Genera los carácteres\n",
    "for i in range(1000):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  sys.stdout.write(result)\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nHecho.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando este ejemplo primero se produce la semilla aleatoria seleccionada, luego cada carácter a medida que se genera. Por ejemplo, a continuación se muestran los resultados de una ejecución de este generador de texto. La semilla aleatoria fue:  \n",
    "  \n",
    "*Semilla: \" plar tan fuerte como el viento de invierno. sopló y sopló, pero la casita de ladrillos era muy resis  \"*  \n",
    "  \n",
    "El texto generado con la semilla aleatoria (limpiado para presentación) fue:  \n",
    "  \n",
    "*-¡uuién teme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  ee ceeeeaa  -¡uui neme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  ee ceeeeaa  -¡uuiéé teme al lobo feroz, al lobo feroz! a  ¡uui ne mabo  le meeee  --¡uuiéé teme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  ee ceeeeaa  -¡uuiéé teme al lobo feroz, al lobo feroz! a  ¡uui ne mabo  le meeee  --¡uuiéé teme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  ee ceeeeaa  -¡uuiéé teme al lobo feroz, al lobo feroz! a  ¡uui ne mabo  le meeee  --¡uuiéé teme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  ee ceeeeaa  -¡uuiéé teme al lobo feroz, al lobo feroz! a  ¡uui ne mabo  le meeee  --¡uuiéé teme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  ee ceeeeaa  -¡uuiéé teme al lobo feroz, al lobo feroz! a  ¡uui ne mabo  le meeee  --¡uuiéé teme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  ee ceeeeaa  -¡uuiéé teme al lobo feroz, al lobo feroz! a  ¡uui ne mabo  le meeee  --¡uuiéé teme al lobo feroz, al lobo feroz! -  ¡uui ne me lo  e  \n",
    "Hecho.*  \n",
    "  \n",
    "Podemos notar algunas observaciones sobre el texto generado.  \n",
    "  \n",
    "Por lo general, se ajusta al formato de línea observado en el texto original de menos de 80 caracteres antes de una nueva línea.\n",
    "Algunas de las palabras en secuencia tienen sentido, pero muchas no lo tienen.  \n",
    "El hecho de que este modelo del libro, basado en caracteres, produzca resultados como estos es muy impresionante. Da una idea de las capacidades de aprendizaje de las redes LSTM. Los resultados son no perfectos.  \n",
    "  \n",
    "En el siguiente punto analizaremos la mejora de la calidad de los resultados mediante el desarrollo de un una red LSTM mucho más grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Recurrente LSTM más grande\n",
    "Ahora vamos ha hacer los mismo creando una red mucho más grande. Mantendremos el mismo número de unidades de memoria en 256, pero añadiremos una segunda capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define la LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También cambiaremos el nombre del peso de los puntos de control para que podamos determinar la diferencia entre los pesos de esta red y el anterior (agregando la palabra más grande en el nombre del archivo):\n",
    "\n",
    "**filename=\"pesos-los3grande-303-30-2.3081.hdf5\"**  \n",
    "  \n",
    "Por último, aumentaremos el número de epoch de formación de 50 a 150 y reduciremos el tamaño del lote de 128 a 64 para dar a la red más oportunidades de actualización y aprendizaje. El código completo sería (con todos los pasos vistos):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total caracters:  3139\n",
      "Total vocabulario:  38\n",
      "Total patrones:  3039\n",
      "Epoch 1/150\n",
      "3039/3039 [==============================] - 46s 15ms/step - loss: 3.0710\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.07103, saving model to pesos-los3grandes-303-01-3.0710.hdf5\n",
      "Epoch 2/150\n",
      "3039/3039 [==============================] - 65s 21ms/step - loss: 2.9907\n",
      "\n",
      "Epoch 00002: loss improved from 3.07103 to 2.99070, saving model to pesos-los3grandes-303-02-2.9907.hdf5\n",
      "Epoch 3/150\n",
      " 832/3039 [=======>......................] - ETA: 48s - loss: 3.0061"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c5e98896be56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#Ajuste del modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Anaconda/anaconda3/envs/data/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Anaconda/anaconda3/envs/data/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/envs/data/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/Anaconda/anaconda3/envs/data/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Grande LSTM Network para generar texto 3 cerditos\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "#Cargamos el texto y lo pasamos a minuscula\n",
    "filename = \"los3.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "#Sumarizamos los datos cargados\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total caracters: \", n_chars)\n",
    "print(\"Total vocabulario: \", n_vocab)\n",
    "#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = raw_text[i:i + seq_length]\n",
    "  seq_out = raw_text[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patrones: \", n_patterns)\n",
    "#Remodelar X para que sea [muestras, pasos de tiempo, características]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "#Normalizacion\n",
    "X = X / float(n_vocab)\n",
    "#Codificacion en caliente con la variable de salida\n",
    "y = np_utils.to_categorical(dataY)\n",
    "#Se define el LSTM modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Se define el checkpoint\n",
    "filepath=\"pesos-los3grandes-303-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "#Ajuste del modelo\n",
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ejecución de este ejemplo lleva algún tiempo, dependiendo de tu ordenador. Después de ejecutar este ejemplo, se puede lograr una pérdida de aproximadamente 0.018. Por ejemplo, el mejor resultado que obtuve al ejecutar este modelo se almacenó en un archivo de punto de control con el nombre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"pesos-los3grandes-303-150-0.0105.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en el punto anterior, podemos utilizar este mejor modelo de la ejecución para generar texto. El único cambio que necesitamos hacer en el script de generación de texto del punto anterior está en la especificación de la topología de la red y desde qué archivo sembrar los pesos de la red. El código completo sería (con todos los pasos vistos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  3139\n",
      "Total Vocab:  38\n",
      "Total Patterns:  3039\n",
      "Semilla:\n",
      "\" no, no te vamos a abrir.  - pues si no me abrís... ¡soplaré y soplaré y la casita derribaré!  y se p \"\n",
      "uso a soplar tan fuerte como el viento de invierno. sopló y sopló, pero la casita de ladrillos era muy resistente y no conseguía derribarla. decidió trepar por la pared y entrar por la chimenea. se deslizó hacia abajo... y cayó en el caldero donde el cerdito mayor estaba hirviendo sopa de nabos. escaldado y con el estómago vacío salió huyendo hacia el lago. los cerditos no lo volvieron a ver.  el mayor de ellos regañó a los otros dos por haber sido tan perezosos y poner en peligro sus propias vidas, y si algún día vais por el bosque y veis tres cerdos, sabréis que son los tres cerditos porque les gusta cantar:  - ¡quién teme al lobo feroz, al lobo, al lobo!  - ¡quién teme al lobo feroz, al lobo feroz! - cantaban desde dentro los cerditos. el lobo estaba realmente enfadado y hambriento, y ahora deseaba comerse a los tres cerditos más que nunca, y frente a la puerta dijo:  - ¡cerditos, abridme la puerta! - no, no, no, no te vamos a abrir.  - pues si no me abrís... ¡soplaré y soplaré y la\n",
      "Hecho.\n"
     ]
    }
   ],
   "source": [
    "#Carga de la red LSTM grande para generar texto\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "#Cargamos el texto y lo pasamos a minuscula\n",
    "filename = \"los3.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "#Sumarizamos los datos cargados\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "#Remodelar X para que sea [muestras, pasos de tiempo, características]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "#Normalizacion\n",
    "X = X / float(n_vocab)\n",
    "#Codificacion en caliente de la variable de salida\n",
    "y = np_utils.to_categorical(dataY)\n",
    "#Define la LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Carga de los pesos de la red\n",
    "filename=\"pesos-los3grandes-303-150-0.0105.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Toma una semilla aleatoria\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Semilla:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "#Genera los carácteres\n",
    "for i in range(1000):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  sys.stdout.write(result)\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nHecho.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de ejecución de este script de generación de texto produce la salida a continuación. El texto semilla elegido al azar fue:  \n",
    "  \n",
    "Semilla:\n",
    "*\" l más pequeño-, la paja es blanda y se puede sujetar con facilidad. terminaré muy pronto y podré ir  \"*  \n",
    "  \n",
    "El texto generado con la semilla (limpiado para presentación) fue:\n",
    "  \n",
    "*a jugar. el hermano mediano decidió que su casa sería de madera:  - puedo encontrar un montón de madera por los alrededores - explicó \n",
    "a sus hermanos, - construiré mi casa en un santiamén con todos estos troncos y me iré también a jugar.  cuando las tres casitas estuvieron terminadas, los cerditos cantaban y bailaban en la puerta, felices por haber acabado con el problema:  -¡quién teme al lobo feroz, al lobo, al lobo!  - ¡quién teme al lobo feroz, al lobo feroz! - cantaban desde dentro los cerditos. el lobo estaba realmente enfadado y hambriento, y ahora deseaba comerse a los tres cerditos más que nunca, y frente a la puerta dijo:  - ¡cerditos, abridme la puerta! - no, no, no, no te vamos a abrir.  - pues si no me abrís... ¡soplaré y soplaré y la casita derribaré!  y se puso a soplar tan fuerte como el viento de invierno. sopló y sopló, pero la casita de ladrillos era muy resistente y no conseguía derribarla. decidió trepar por la pared y entrar por la chimenea. se deslizó hacia abajo  \n",
    "Hecho.*  \n",
    "  \n",
    "Podemos ver que generalmente hay menos errores de ortografía y el texto parece más realista. Estos son mejores resultados, pero hay todavía se podría mejorar mucho más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas de extensión para mejorar el modelo\n",
    "A continuación se muestran ideas que se pueden investigar para mejorar aún más el modelo:\n",
    "\n",
    "- Predecir menos de 1.000 caracteres como salida para una semilla dada.\n",
    "- Eliminar toda la puntuación del texto fuente y, por lo tanto, del vocabulario de los modelos.\n",
    "- Prueba con una codificación en caliente para las secuencias de entrada.\n",
    "- Aumentar el número de epoch de entrenamiento a 100 o cientos.\n",
    "- Añadir más unidades de memoria a las capas y/o más capas.\n",
    "- Experimentar con los factores de escala (temperatura) al interpretar las probabilidades de predicción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
